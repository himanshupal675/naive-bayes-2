{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5acd9f-394c-4095-9695-2c352d8bf6f0",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7f22f-b4fb-4824-acd4-1c2e6b12fd60",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, you can use conditional probability. You want to calculate \\(P(\\text{Smoker} | \\text{Uses Insurance})\\).\n",
    "\n",
    "You are given the following information:\n",
    "\n",
    "- \\(P(\\text{Uses Insurance}) = 0.70\\), which is the probability that an employee uses the health insurance plan.\n",
    "- \\(P(\\text{Smoker} | \\text{Uses Insurance}) = 0.40\\), which is the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "You can use the formula for conditional probability:\n",
    "\n",
    "\\[P(\\text{A} | \\text{B}) = \\frac{P(\\text{A} \\cap \\text{B})}{P(\\text{B})}\\]\n",
    "\n",
    "In this case, A represents \"Smoker\" and B represents \"Uses Insurance.\"\n",
    "\n",
    "So, plug in the values:\n",
    "\n",
    "\\[P(\\text{Smoker} | \\text{Uses Insurance}) = \\frac{P(\\text{Smoker} \\cap \\text{Uses Insurance})}{P(\\text{Uses Insurance})}\\]\n",
    "\n",
    "Now, calculate \\(P(\\text{Smoker} \\cap \\text{Uses Insurance})\\) using the information you have:\n",
    "\n",
    "\\[P(\\text{Smoker} \\cap \\text{Uses Insurance}) = P(\\text{Uses Insurance}) \\cdot P(\\text{Smoker} | \\text{Uses Insurance})\\]\n",
    "\n",
    "Substitute the values:\n",
    "\n",
    "\\[P(\\text{Smoker} \\cap \\text{Uses Insurance}) = 0.70 \\cdot 0.40 = 0.28\\]\n",
    "\n",
    "Now, you can calculate the conditional probability:\n",
    "\n",
    "\\[P(\\text{Smoker} | \\text{Uses Insurance}) = \\frac{0.28}{0.70} = 0.4\\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is \\(0.4\\) or \\(40%\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ad3f7-b6ca-46f9-a299-c5be59b87f02",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350573b6-f44c-4a3f-8cde-fb0f74221f0d",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes classifier, and they are commonly used in different types of text classification problems. Here are the key differences between them:\n",
    "\n",
    "**1. Nature of Features:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** This variant is suitable for binary data, where features are either present (1) or absent (0). It's commonly used for text classification tasks where you represent documents as binary feature vectors, indicating the presence or absence of specific terms or words. In Bernoulli Naive Bayes, the focus is on whether a feature occurs or not, regardless of its frequency.\n",
    "\n",
    "- **Multinomial Naive Bayes:** This variant is designed for count-based data, where features represent the frequency of occurrences of specific events or terms. It's commonly used in text classification tasks where you have features like word counts or term frequencies in documents. Multinomial Naive Bayes takes into account both the presence and frequency of features.\n",
    "\n",
    "**2. Feature Probability Distribution:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** It assumes a Bernoulli distribution for each feature, meaning that features are treated as binary random variables with a probability of success (presence) and a probability of failure (absence). It calculates the likelihood of features being present or absent in each class.\n",
    "\n",
    "- **Multinomial Naive Bayes:** It assumes a multinomial distribution for the feature counts, where each feature follows a multinomial distribution based on its frequency. It calculates the likelihood of observing a particular count or frequency of features in each class.\n",
    "\n",
    "**3. Use Cases:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** It is commonly used for tasks such as document classification, spam email detection, sentiment analysis, and any problem where you want to capture the presence or absence of certain features in documents.\n",
    "\n",
    "- **Multinomial Naive Bayes:** It is well-suited for text classification problems where you have count-based features, such as the number of times specific words appear in documents. It is widely used in tasks like document categorization, topic modeling, and text mining.\n",
    "\n",
    "**4. Handling of Zero Counts:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** It can handle features with zero counts, as it only considers the presence or absence of features. However, it might not capture the nuances of feature frequency.\n",
    "\n",
    "- **Multinomial Naive Bayes:** It directly incorporates feature counts, so zero counts can be problematic. Techniques like Laplace smoothing (additive smoothing) are often used to handle zero counts and prevent probabilities from becoming zero.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the specific requirements of your text classification problem. If your data is binary and you are interested in the presence or absence of features, Bernoulli Naive Bayes is a good choice. If your data involves feature counts or frequencies, Multinomial Naive Bayes is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866ee36-40f1-4b4d-890a-eb1760a211be",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f98294-8a78-4374-97ac-35da4fc32146",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes variants, typically assumes that each feature is binary, representing the presence (1) or absence (0) of a specific attribute or event. In the context of Bernoulli Naive Bayes, missing values are generally treated as the absence of the feature. Here's how Bernoulli Naive Bayes handles missing values:\n",
    "\n",
    "1. **Assumption of Binary Features**: Bernoulli Naive Bayes assumes that each feature is binary, meaning it's either present or absent. Therefore, if a particular feature is missing for an instance, it is treated as if the feature is absent (assigned a value of 0).\n",
    "\n",
    "2. **Presence vs. Absence**: In the context of text classification, Bernoulli Naive Bayes is often used for problems where you represent documents as binary feature vectors, indicating the presence or absence of specific terms or words. If a term is missing in a document, it is considered as not present in that document (assigned a value of 0). If a term is present, it is assigned a value of 1.\n",
    "\n",
    "3. **Conditional Probabilities**: When calculating conditional probabilities in Bernoulli Naive Bayes, it considers both the presence (1) and absence (0) of features. It calculates the likelihood of a feature being present (1) in each class and the likelihood of a feature being absent (0) in each class.\n",
    "\n",
    "4. **Handling Missing Data**: If a feature is missing in an instance, it doesn't affect the calculations directly. Instead, the absence of the feature is taken into account when computing probabilities. This means that instances with missing values are still classified based on the presence or absence of other features.\n",
    "\n",
    "5. **Laplace Smoothing**: To avoid zero probabilities, Bernoulli Naive Bayes often incorporates Laplace smoothing (additive smoothing). Laplace smoothing adds a small constant to the counts of features in each class, which helps avoid issues with zero probabilities and provides more robust classification.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes handles missing values by treating them as the absence (0) of features, consistent with its assumption of binary features. The absence of a feature is considered when calculating probabilities, and techniques like Laplace smoothing are commonly used to handle issues related to zero probabilities. It's important to preprocess your data appropriately and ensure that missing values are appropriately handled before applying the Bernoulli Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415ea58-45a3-4af5-80d3-d6515c9758f3",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e16c5-0fa3-48f8-9732-e0751c062e75",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is suitable for data with continuous features, and it can be adapted for multi-class classification by applying the algorithm to problems with more than two classes.\n",
    "\n",
    "In Gaussian Naive Bayes, each class is assumed to follow a Gaussian (normal) distribution for its features. The key idea is to estimate the mean and variance of the feature values for each class, and then use these estimates to calculate the probability of an instance belonging to each class given its feature values. The class with the highest probability is the predicted class.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "1. **Parameter Estimation**: For each class, estimate the mean and variance of the feature values for each feature. This involves calculating the mean and variance of the feature values for each class in your training data.\n",
    "\n",
    "2. **Class Prior Probabilities**: Estimate the prior probabilities of each class, which can be done by counting the number of instances in each class and dividing by the total number of instances.\n",
    "\n",
    "3. **Conditional Probability**: For a new instance with feature values \\(X_1, X_2, \\ldots, X_n\\), calculate the conditional probability of it belonging to each class using the Gaussian probability density function:\n",
    "\n",
    "   \\[P(Class_i | X_1, X_2, \\ldots, X_n) \\propto P(Class_i) \\cdot \\prod_{j=1}^{n} P(X_j | Class_i)\\]\n",
    "\n",
    "   Here, \\(P(Class_i)\\) is the prior probability of Class \\(i\\), and \\(P(X_j | Class_i)\\) is the probability of feature \\(X_j\\) given Class \\(i\\) based on the Gaussian distribution.\n",
    "\n",
    "4. **Prediction**: Assign the instance to the class with the highest conditional probability.\n",
    "\n",
    "In this way, you can apply Gaussian Naive Bayes to problems with multiple classes. Each class is modeled as having a Gaussian distribution for its feature values, and the algorithm calculates the conditional probabilities of an instance belonging to each class based on these distributions. The class with the highest probability is the predicted class.\n",
    "\n",
    "It's important to note that while Gaussian Naive Bayes can be used for multi-class classification, it makes certain assumptions about the distribution of feature values. These assumptions may not always hold in practice, so it's advisable to assess the performance of the algorithm on your specific dataset and consider other classification methods as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da488616-497a-42b9-a96e-ae65185dd190",
   "metadata": {},
   "source": [
    "## Answer 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb081f59-795f-412e-93b5-405eb046fa93",
   "metadata": {},
   "source": [
    "I can provide you with a step-by-step guide on how to implement and evaluate Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn in Python. However, I cannot directly execute the code or access external websites to download the dataset. You'll need to perform these steps on your local machine. Here's a general outline of what you need to do:\n",
    "\n",
    "**Step 1: Download the Spambase Dataset**\n",
    "\n",
    "1. Go to the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Spambase.\n",
    "2. Download the dataset file (e.g., \"spambase.data\") from the provided link.\n",
    "\n",
    "**Step 2: Import Libraries**\n",
    "\n",
    "You'll need to import the necessary libraries in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "```\n",
    "\n",
    "**Step 3: Load and Prepare the Data**\n",
    "\n",
    "Load the dataset into a Pandas DataFrame and prepare the features (X) and target labels (y).\n",
    "\n",
    "```python\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "\n",
    "# Split the data into features (X) and target labels (y)\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Target labels\n",
    "```\n",
    "\n",
    "**Step 4: Implement and Evaluate Naive Bayes Classifiers**\n",
    "\n",
    "Now, implement and evaluate the three Naive Bayes classifiers using 10-fold cross-validation. You can use the `cross_val_score` function from scikit-learn to perform this:\n",
    "\n",
    "```python\n",
    "# Create instances of the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform cross-validation and calculate performance metrics\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for classifier, classifier_name in zip([bernoulli_nb, multinomial_nb, gaussian_nb],\n",
    "                                       ['Bernoulli Naive Bayes', 'Multinomial Naive Bayes', 'Gaussian Naive Bayes']):\n",
    "    print(f\"Classifier: {classifier_name}\")\n",
    "    for metric in scoring_metrics:\n",
    "        scores = cross_val_score(classifier, X, y, cv=10, scoring=metric)\n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"{metric.capitalize()}: {mean_score:.4f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "```\n",
    "\n",
    "**Step 5: Discussion and Conclusion**\n",
    "\n",
    "- In your discussion, analyze the results obtained for each classifier in terms of accuracy, precision, recall, and F1 score.\n",
    "- Identify which variant of Naive Bayes performed the best and provide reasons for your observations.\n",
    "- Discuss any limitations or challenges you encountered during the analysis.\n",
    "- In your conclusion, summarize your findings and provide suggestions for future work, such as exploring different preprocessing techniques, hyperparameter tuning, or trying other classification algorithms.\n",
    "\n",
    "Remember to replace the dataset filename (\"spambase.data\") and adjust the code as needed for any specific data preprocessing steps or additional analysis you may want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e46ad4-16e9-444a-b6a4-4fca00a616b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
